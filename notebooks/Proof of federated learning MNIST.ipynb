{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a Proof of Federated Learning for the MNIST dataset\n",
    "\n",
    "In this notebook we will use a Proof of Federated Learning (PoFL) blockchain architecture to train a model on the MNIST dataset. PoFL is a consensus mechanism designed for federated learning, contrary to traditional consensus mechanism such as Proof of Work (PoW) or Proof of Stake (PoS) which are designed for a generic blockchain. This notebook will be straightforward since we will not dive into the implementation of PoFL in `flexBlock`. \n",
    "\n",
    "### How does PoFL work?\n",
    "In a PoFL blockchain, we must differentiate between two types of nodes: the miners and the clients. The miners are responsible for creating new blocks, running the consensus mechanism and validating the blocks. The clients are the data owners, they are the ones who train the model. Every client is allocated to a miner, and each miner with its clients will act as an independent pool. The miner with the best model (i.e. the model with the highest accuracy) will propagate its model to the other miners. The other miners will validate the model and if it is better than their own, they will accept it.\n",
    "\n",
    "\n",
    "Let's start by writting some boilerplate code. We will use torch for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use the mnist dataset. In order to load the dataset, we can use the `load` function from flex which gives us access to federated datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flex.datasets import load\n",
    "from torchvision import transforms\n",
    "\n",
    "flex_dataset, test_data = load(\"federated_emnist\", return_test=True, split=\"digits\")\n",
    "\n",
    "mnist_transforms = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will define the model and the function for init the servers models. This is standard `flex` code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from flex.pool import init_server_model\n",
    "from flex.model import FlexModel\n",
    "\n",
    "\n",
    "\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "@init_server_model\n",
    "def build_server_model():\n",
    "    server_flex_model = FlexModel()\n",
    "\n",
    "    server_flex_model[\"model\"] = SimpleNet()\n",
    "    # Required to store this for later stages of the FL training process\n",
    "    server_flex_model[\"criterion\"] = torch.nn.CrossEntropyLoss()\n",
    "    server_flex_model[\"optimizer_func\"] = torch.optim.Adam\n",
    "    server_flex_model[\"optimizer_kwargs\"] = {}\n",
    "    return server_flex_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to create our pool. `flexBlock` has a ready-to-use implementation of PoFL. We just need to import it from the `pool` module and create a new instance of it. It requires a dataset, a function for initializing the models and the ammount of miners that we want in our blockchain. The clients will be split evenly between the miners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from flexBlock.pool import PoFLBlockchainPool\n",
    "\n",
    "pool = PoFLBlockchainPool(fed_dataset=flex_dataset, init_func=build_server_model, number_of_miners=10)\n",
    "\n",
    "clients = pool.clients\n",
    "servers = pool.servers\n",
    "aggregators = pool.aggregators\n",
    "\n",
    "print(\n",
    "    f\"Number of nodes in the pool {len(pool)}: {len(servers)} miners plus {len(clients)} clients. The server is also an aggregator\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then define a function for deploying the miners (server) model to the clients. Since we are on `flexBlock` we will use the `deploy_miner_model` function from the `pool.decorators` module. If you have a function for doing this in `flex` remember that you can reuse it by wrapping it with the `deploy_server_to_miner_wrapper` function from the `pool.primitives` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "from flexBlock.pool.decorators import deploy_miner_model\n",
    "\n",
    "@deploy_miner_model\n",
    "def copy_server_model_to_clients(server_flex_model: FlexModel):\n",
    "    return copy.deepcopy(server_flex_model)\n",
    "\n",
    "\n",
    "servers.map(copy_server_model_to_clients, clients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define an standard training function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flex.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def train(client_flex_model: FlexModel, client_data: Dataset):\n",
    "    train_dataset = client_data.to_torchvision_dataset(transform=mnist_transforms)\n",
    "    client_dataloader = DataLoader(train_dataset, batch_size=20)\n",
    "    model = client_flex_model[\"model\"]\n",
    "    optimizer = client_flex_model[\"optimizer_func\"](\n",
    "        model.parameters(), **client_flex_model[\"optimizer_kwargs\"]\n",
    "    )\n",
    "    model = model.train()\n",
    "    model = model.to(device)\n",
    "    criterion = client_flex_model[\"criterion\"]\n",
    "    for _ in range(1):\n",
    "        for imgs, labels in client_dataloader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(imgs)\n",
    "            loss = criterion(pred, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function for collecting our weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flexBlock.pool import send_weights_to_miner\n",
    "\n",
    "@send_weights_to_miner\n",
    "def get_clients_weights(client_flex_model: FlexModel):\n",
    "    weight_dict = client_flex_model[\"model\"].state_dict()\n",
    "    return [weight_dict[name] for name in weight_dict]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a function for updating the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flex.pool import set_aggregated_weights\n",
    "\n",
    "\n",
    "@set_aggregated_weights\n",
    "def set_agreggated_weights_to_server(server_flex_model: FlexModel, aggregated_weights):\n",
    "    with torch.no_grad():\n",
    "        weight_dict = server_flex_model[\"model\"].state_dict()\n",
    "        for layer_key, new in zip(weight_dict, aggregated_weights):\n",
    "            weight_dict[layer_key].copy_(new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will need a function for evaluating the global model of each miner. This function will take the global model of each miner and compute the accuracy in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_global_model(server_flex_model: FlexModel, test_data: Dataset):\n",
    "    model = server_flex_model[\"model\"]\n",
    "    model.eval()\n",
    "    test_acc = 0\n",
    "    total_count = 0\n",
    "    model = model.to(device)\n",
    "    # get test data as a torchvision object\n",
    "    test_dataset = test_data.to_torchvision_dataset(transform=mnist_transforms)\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset, batch_size=256, shuffle=True, pin_memory=False\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_dataloader:\n",
    "            total_count += target.size(0)\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            test_acc += pred.eq(target.data.view_as(pred)).long().cpu().sum().item()\n",
    "\n",
    "    test_acc /= total_count\n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we define a training loop. When we call the `aggregate` method we see that we pass the `eval_function` for computing accuracy, the `eval_dataset` where we want to compute the accuracy and a threshold for the accuracy. We see that the `aggregate` method will return a boolean value indicating if we reached an aggregation, which means that at least one model has surpassed the threshold and by so communicated to the other miners, or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flex.pool import fed_avg\n",
    "\n",
    "def train_until_acc(acc: float):\n",
    "    aggregated = False\n",
    "    i = 0\n",
    "    while not aggregated:\n",
    "        i = i + 1\n",
    "        print(f\"\\nRunning round: {i}\")\n",
    "        # Deploy the server model to the selected clients\n",
    "        pool.servers.map(copy_server_model_to_clients, clients)\n",
    "        selected_clients = pool.clients.select(20)\n",
    "        # Each selected client trains her model\n",
    "        selected_clients.map(train)\n",
    "        # The aggregador collects weights from the selected clients and aggregates them\n",
    "        pool.aggregators.map(get_clients_weights, selected_clients)\n",
    "        aggregated = pool.aggregate(fed_avg, set_agreggated_weights_to_server, eval_function=evaluate_global_model, eval_dataset=test_data, accuracy=acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can run our experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_until_acc(acc=0.6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
